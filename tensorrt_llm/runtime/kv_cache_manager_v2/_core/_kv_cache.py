import enum
import warnings
import weakref
from collections.abc import Sequence
from typing import Callable, Iterator, NewType

from .._common import BlockOrdinal, CudaStream, LayerId, Priority, TokenIdExt
from .._config import DataRole
from .._life_cycle_registry import LifeCycle, LifeCycleId
from .._page import _PageHolder, _SharedPageLock
from .._utils import (CachedCudaEvent, TypedIndexList, unwrap_optional,
                      unwrap_weakref)
from ._kv_cache_manager import KVCacheManager

BlockPage = _SharedPageLock | _PageHolder | None
BeamIndex = NewType("BeamIndex", int)


# The _KVCache holds unique/shared ownership of memory blocks. On deletion, the ownership if destroys and KVCacheManager takes control of them. A KV cache maintains three lengths:
#  1.	num_committed_tokens: the number of tokens that are finalized, immutable and ready for reuse.
#  2.	history_length: a cursor separating history and the space for next input tokens. History tokens are defined as tokens without query data for the next inference step. For SWA layers, it decides which blocks are out-of-window and can be evicted/dropped. In most cases, you don’t need to touch history_length as it’s automatically bumped by the increase of num_committed_tokens, except a few cases:
#     a.	Beam search where we can’t commit tokens generated by the last step. But it still makes sense to evict uncommitted pages for SWA layers to save memory.
#     b.	Disaggregated serving with SWA and the reusable tokens are in the other server. We need to reserve space for history. Knowing history_length helps us accurately decide which blocks needs to be allocated. Then users only transfer data for what is needed.
#     c.	Multi-round conversation with chain of thoughts (CoT) and excluding CoT tokens for the next round. In this case, users should not commit tokens starting from CoT. Then history_length needs to be explicitly bumped.
#  3.	capacity: the number of tokens that can be stored in the KV cache. It should include the number of both historical tokens and input tokens for the next inference step, no matter if it’s prefill, chunked prefill or generation without/without speculative decoding. For tree-based speculative decoding, the number of input tokens here should be the flatten draft length. For beam search, multiple candidate tokens at the same position are counted as one.
# num_committed_tokens <= history_length <= capacity always holds. A newly created KV cache has all three lengths equal to the number of reused tokens.
# @TODO: in __del__, we should check if committed pages are usable for SWA cases. e.g. all pages are dropped except the last one. The last one is not usable.
class _KVCache:
    __slots__ = ('_manager', 'get_priority', 'status', '_beam_width',
                 '_cuda_stream', 'capacity', 'history_length',
                 'num_committed_tokens', '_pages', '_finish_event')

    class Status(enum.IntEnum):
        ACTIVE = 0
        SUSPENDED = 1
        CLOSED = 2

    _manager: weakref.ref[KVCacheManager]
    get_priority: Callable[[BlockOrdinal, LifeCycle], Priority]
    _cuda_stream: CudaStream
    _status: Status
    _beam_width: int
    capacity: int
    history_length: int
    num_committed_tokens: int

    _pages: TypedIndexList[BlockOrdinal,
                           TypedIndexList[BeamIndex, TypedIndexList[LifeCycleId,
                                                                    BlockPage]]]
    # set when switch away from ACTIVE, cleared when switching to ACTIVE.
    _finish_event: CachedCudaEvent | None

    def __init__(self, manager: KVCacheManager, stream: CudaStream,
                 lora_task_id: int | None,
                 input_tokens: Sequence[TokenIdExt] | None,
                 custom_priority_callback: Callable[[BlockOrdinal, LifeCycle],
                                                    Priority], suspended: bool):
        self._manager = weakref.ref(manager)
        self._cuda_stream = stream
        self._status = self.Status.SUSPENDED
        self._beam_width = 1
        self.capacity = 0
        self.history_length = 0
        self.num_committed_tokens = 0
        raise NotImplementedError("Not implemented")

    @property
    def manager(self) -> KVCacheManager:
        return unwrap_weakref(self._manager)

    @property
    def cuda_stream(self) -> CudaStream:
        return self._cuda_stream

    @property
    def finish_event(self) -> CachedCudaEvent:
        'Event recorded when switching from active to suspended/closed state.'
        return unwrap_optional(self._finish_event)

    # destroy ownership of memory blocks, so KV cache manager can decide to evict or drop them.
    def close(self):
        if self.status == self.Status.CLOSED:
            return
        if self.num_committed_tokens % self.tokens_per_block != 0:
            ordinal = self.num_committed_tokens // self.tokens_per_block
            self._commit_block(ordinal, is_last=True)
        if self.status == self.Status.ACTIVE:
            # suspend first, just to record finish_event.
            self.suspend()
        # make last block first droppable first.
        while self._pages:
            self._pages.pop()
        self._status = self.Status.CLOSED

    def __del__(self):
        self.close()

    @property
    def beam_width(self) -> int:
        return self._beam_width

    # beam_width > 1 is only for generation. If decreasing beam_width, uncommitted data in blocks for (beam_index >= beam_width) will be lost.
    @beam_width.setter
    def beam_width(self, beam_width: int):
        self._beam_width = beam_width
        raise NotImplementedError("Not implemented")

    # Get the indices of memory blocks for each beam.
    # Due to constraints of the current kernels, K/V data blocks and the correspondding quant scale blocks
    # share the same indices, so the output for DataRole.KEY_DATA and DataRole.KEY_BLOCK_SCALE are the same.
    def get_page_indices(
        self,
        layer_id: LayerId,
        data_role: DataRole,
        beam_id: BeamIndex = BeamIndex(0)) -> Iterator[int]:
        storage = self.manager._storage
        lc = storage.get_buffer_attr(layer_id, data_role).life_cycle_id
        pages = (unwrap_optional(p[beam_id][lc]).page for p in self._pages)
        return self.manager._storage.get_page_indices(layer_id, data_role,
                                                      pages)

    # Currently always equals to page size. In the future, that will change when kernels support page stride.
    def get_page_stride(self, layer_id: LayerId, data_role: DataRole) -> int:
        storage = self.manager._storage
        attr = storage.get_buffer_attr(layer_id, data_role)
        return attr.size

    # reserve space for next inference. Request new blocks from KVCacheManager if necessary.
    # if delta_capacity > 0 and beam_width > 1, blocks containing new tokens should be allocated for each beam.
    # delta_capacity < 0 decreases the capacity and may destroy stale blocks to KVCacheManager (if not used by other requests).
    # delta_capacity < 0 cannot remove historical or committed tokens.
    # delta_history_length >= 0.
    # If delta_history_length != 0, also move the cursor for history, which may trigger out-of-window block
    # eviction/dropping for SWA layers. If we use two separate APIs for capacity and history length, we
    # will need to increase capacity first to maintain capacity >= history_length. But then we may have
    # a middle state (between two APIs) where we use more pages than necessary for SWA layers. So we use a
    # single API to avoid this.
    def grow(self, delta_capacity: int, delta_history_length: int = 0) -> bool:
        ...

    # Get the current capacity in number of tokens.
    def get_capacity(self) -> int:
        ...

    # Get the current history length in number of tokens. history_length decides how many blocks needs to be
    # in GPU memory for SWA layers.
    def get_history_length(self) -> int:
        ...

    # notify KV cache manager that we have some finalized/accepted tokens. If a block becomes full, also commit the block for reuse.
    # In case of beam search, this should be called only with finalized (converged) tokens, and the token data must be in the 0th beam.
    # We'll destroy memory blocks for other beams if the whole block is full and committed.
    # Committed tokens are always history, so history_length will be automatically updated to maintain
    # (num_committed_tokens <= history_length). Note that history_length increase may trigger out-of-window
    # block eviction/dropping for SWA layers.
    # beam_search_indices: indices indicating which candidate to choose for each token. A block with all tokens committed will be unified to one memory page and the other memory pages are dropped. Only for beam search.
    def commit(self,
               accepted_input_tokens: Sequence[TokenIdExt],
               beam_search_indices: Sequence[int] | None = None):
        ...

    # Note that the tokens may not be ready yet, if the event passed to the past commit() calls are not yet signaled.
    def get_num_committed_tokens(self) -> int:
        ...

    # Users promise to not commit any more tokens. For cases where we shouldn't reuse generated tokens (eg. CoT), this helps us drop (instead of evict) out-of-window blocks for SWA layers.
    def stop_committing(self):
        ...

    # Suspend, allow the KV cache manager to evict buffers from GPU, but don't drop them.
    # suspend+resume allows us to implement dynamic batch size. May also be used to support HSTU model.
    def suspend(self, evict_immediately: bool = False) -> bool:
        ...

    # Resume, promote buffers to GPU memory.
    def resume(self) -> bool:
        ...

    @property
    def status(self) -> Status:
        return self._status

    # Wait until the whole helix group arrives at the same phase.
    # When calling this, all ranks must have the same KV cache lengths (capacity, history_length, num_committed_tokens), committed tokens, beam_width, and status. Otherwise the behavior is undefined.
    # This is for helix parallelism. Returns immediately if not using helix parallelism.
    def helix_sync(self):
        raise NotImplementedError("Not implemented")

    # From which rank the sequence starts. This API is required only if we support helix sequence starting from arbitrary rank.
    def helix_seq_start_rank(self) -> int:
        raise NotImplementedError("Not implemented")

    def _page(
        self,
        block_ordinal: BlockOrdinal,
        life_cycle: LifeCycleId,
        beam_index: BeamIndex = BeamIndex(0)) -> BlockPage:
        return self._block(block_ordinal, beam_index)[life_cycle]

    def _block(
        self, block_ordinal: BlockOrdinal, beam_index: BeamIndex = BeamIndex(0)
    ) -> TypedIndexList[LifeCycleId, BlockPage]:
        return self._pages[block_ordinal][beam_index]

    def _commit_block(self, ordinal: BlockOrdinal, is_last: bool = False):
        'Commit the block for reuse. Block must be full of tokens except for the last block.'
        if not is_last:
            assert self.num_committed_tokens >= self.manager.tokens_per_block * ordinal
        # convert uncommit pages to committed pages and create a new block in the radix tree.

        raise NotImplementedError("Not implemented")
        if is_last:
            warnings.warn(
                "[KVCacheManager] Not Implemented: check if the last committed pages are usable, in case some prior pages are already dropped. For SWA, this can be done only when _KVCache terminates."
            )

    @property
    def tokens_per_block(self) -> int:
        return self.manager.tokens_per_block
