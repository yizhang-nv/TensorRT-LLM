import enum
from collections.abc import Sequence

from .._common import LayerId, TokenIdExt
from .._config import DataRole


# The _KVCache holds unique/shared ownership of memory blocks. On deletion, the ownership if destroys and KVCacheManager takes control of them. A KV cache maintains three lengths:
#  1.	num_committed_tokens: the number of tokens that are finalized, immutable and ready for reuse.
#  2.	history_length: a cursor separating history and the space for next input tokens. History tokens are defined as tokens without query data for the next inference step. For SWA layers, it decides which blocks are out-of-window and can be evicted/dropped. In most cases, you don’t need to touch history_length as it’s automatically bumped by the increase of num_commited_tokens, except a few cases:
#     a.	Beam search where we can’t commit tokens generated by the last step. But it still makes sense to evict uncommitted pages for SWA layers to save memory.
#     b.	Disaggregated serving with SWA and the reusable tokens are in the other server. We need to reserve space for history. Knowing history_length helps us accurately decide which blocks needs to be allocated. Then users only transfer data for what is needed.
#     c.	Multi-round conversation with chain of thoughts (CoT) and excluding CoT tokens for the next round. In this case, users should not commit tokens starting from CoT. Then history_length needs to be explicitly bumped.
#  3.	capacity: the number of tokens that can be stored in the KV cache. It should include the number of both historical tokens and input tokens for the next inference step, no matter if it’s prefill, chunked prefill or generation without/without speculative decoding. For tree-based speculative decoding, the number of input tokens here should be the flatten draft length. For beam search, multiple candidate tokens at the same position are counted as one.
# num_committed_tokens <= history_length <= capacity always holds. A newly created KV cache has all three lengths equal to the number of reused tokens.
class _KVCache:

    class Status(enum.IntEnum):
        ACTIVE = 0
        SUSPENDED = 1
        CLOSED = 2

    status: Status

    # destroy ownership of memory blocks, so KV cache manager can decide to evict or drop them.
    def close(self):
        if self.status != self.Status.CLOSED:
            ...
            self.status = self.Status.CLOSED

    def __del__(self):
        self.close()

    # beam_width > 1 is only for generation. If decreasing beam_width, uncommitted data in blocks for (beam_index >= beam_width) will be lost.
    def set_beam_width(self, beam_width: int = 1):
        ...

    # Get the base address of the memory pool holding pages for the given layer and data role.
    def get_mem_pool_base_address(self, layer_id: LayerId,
                                  data_role: DataRole) -> int:
        ...

    # Get the indices of memory blocks for each beam.
    # Due to constraints of the current kernels, K/V data blocks and the correspondding quant scale blocks
    # share the same indices, so the output for DataRole.KEY_DATA and DataRole.KEY_BLOCK_SCALE are the same.
    def get_page_indices(self,
                         layer_id: LayerId,
                         data_role: DataRole,
                         beam_id: int = 0) -> list[int]:
        ...

    # reserve space for next inference. Request new blocks from KVCacheManager if necessary.
    # if delta_capacity > 0 and beam_width > 1, blocks containing new tokens should be allocated for each beam.
    # delta_capacity < 0 decreases the capacity and may destroy stale blocks to KVCacheManager (if not used by other requests).
    # delta_capacity < 0 cannot remove historical or committed tokens.
    # delta_history_length >= 0.
    # If delta_history_length != 0, also move the cursor for history, which may trigger out-of-window block
    # eviction/dropping for SWA layers. If we use two separate APIs for capacity and history length, we
    # will need to increase capacity first to maintain capacity >= history_length. But then we may have
    # a middle state (between two APIs) where we use more pages than necessary for SWA layers. So we use a
    # single API to avoid this.
    def grow(self, delta_capacity: int, delta_history_length: int = 0) -> bool:
        ...

    # Get the current capacity in number of tokens.
    def get_capacity(self) -> int:
        ...

    # Get the current history length in number of tokens. history_length decides how many blocks needs to be
    # in GPU memory for SWA layers.
    def get_history_length(self) -> int:
        ...

    # notify KV cache manager that we have some finalized/accepted tokens. If a block becomes full, also commit the block for reuse.
    # In case of beam search, this should be called only with finalized (converged) tokens, and the token data must be in the 0th beam.
    # We'll destroy memory blocks for other beams if the whole block is full and committed.
    # Committed tokens are always history, so history_length will be automatically updated to maintain
    # (num_committed_tokens <= history_length). Note that history_length increase may trigger out-of-window
    # block eviction/dropping for SWA layers.
    # beam_search_indices: indices indicating which candidate to choose for each token. A block with all tokens committed will be unified to one memory page and the other memory pages are dropped. Only for beam search.
    def commit(self,
               accepted_input_tokens: Sequence[TokenIdExt],
               beam_search_indices: Sequence[int] | None = None):
        ...

    # Note that the tokens may not be ready yet, if the event passed to the past commit() calls are not yet signaled.
    def get_num_committed_tokens(self) -> int:
        ...

    # Users promise to not commit any more tokens. For cases where we shouldn't reuse generated tokens (eg. CoT), this helps us drop (instead of evict) out-of-window blocks for SWA layers.
    def stop_committing(self):
        ...

    # Suspend, allow the KV cache manager to evict buffers from GPU, but don't drop them.
    # suspend+resume allows us to implement dynamic batch size. May also be used to support HSTU model.
    def suspend(self, evict_immediately: bool = False) -> bool:
        ...

    # Resume, promote buffers to GPU memory.
    def resume(self) -> bool:
        ...

    def get_status(self) -> Status:
        ...

    # Wait until the whole helix group arrives at the same phase.
    # When calling this, all ranks must have the same KV cache lengths (capacity, history_length, num_committed_tokens), committed tokens, beam_width, and status. Otherwise the behavior is undefined.
    # This is for helix parallelism. Returns immediately if not using helix parallelism.
    def helix_sync(self):
        ...

    # From which rank the sequence starts. This API is required only if we support helix sequence starting from arbitrary rank.
    def helix_seq_start_rank(self) -> int:
        ...
