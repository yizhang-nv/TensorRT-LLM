from collections.abc import Callable, Sequence

from .._block_radix_tree import BlockRadixTree
from .._common import (BlockOrdinal, CacheLevel, CacheTier, LayerId, MemAddress,
                       Priority, TokenIdExt)
from .._config import DataRole, KVCacheManagerConfig
from .._eviction_controller import PageStatus
from .._life_cycle_registry import LifeCycle, LifeCycleRegistry
from .._storage._config import create_storage_config
from .._storage_manager import StorageManager
from .._utils import HomoTuple, unwrap_weakref
from ._kv_cache import _KVCache


class KVCacheManager:
    __slots__ = ('_life_cycles', '_radix_tree', '_storage', '__weakref__')
    _life_cycles: LifeCycleRegistry
    _radix_tree: BlockRadixTree
    _storage: StorageManager

    def __init__(self, config: KVCacheManagerConfig):
        self._life_cycles = LifeCycleRegistry(config)
        self._radix_tree = BlockRadixTree(self._life_cycles,
                                          config.tokens_per_block)
        storage_config = create_storage_config(config)
        self._storage = StorageManager(self, storage_config)

    def __del__(self):
        self.clear_reusable_blocks()

    def clear_reusable_blocks(self):
        for ref in self._radix_tree.clear(yield_pages=True):
            assert unwrap_weakref(ref).status == PageStatus.DROPPABLE
            self._storage.exclude_from_eviction(unwrap_weakref(ref))

    # Get the base address of the memory pool holding pages for the given layer and data role.
    def get_mem_pool_base_address(self, layer_id: LayerId,
                                  data_role: DataRole) -> MemAddress:
        return self._storage.get_mem_pool_base_address(layer_id, data_role)

    # Currently always equals to page size. In the future, that will change when kernels support page stride.
    def get_page_stride(self, layer_id: LayerId, data_role: DataRole) -> int:
        attr = self._storage.get_buffer_attr(layer_id, data_role)
        return attr.size

    # lora_task_id: match lora_task_id before matching any tokens.
    # stream: blocks are allocated and made ready in this stream. Later grow() also makes blocks ready in this stream, and later commit() calls also assume data are written in this stream.
    # custom_priority_callback: takes block index and layer sliding window size, returns priority.
    # If priority returned is higher than existing priority for reused blocks, the block priority is updated.
    # Newly created KV cache is suspended. You need to call resume() with a cuda stream to make it active & ready in that stream.
    # Returns None if suspended=False and we don't have enough resource.
    # This call will attempt to reuse KV cache blocks.
    # It's user responsibility to remove the last token from prompts if we need to re-compute the token generated by prefill.

    def create_kv_cache(
        self,
        lora_task_id: int | None = None,
        input_tokens: Sequence[TokenIdExt] | None = None,
        custom_priority_callback: Callable[
            [BlockOrdinal, LifeCycle],
            Priority] = lambda _, __: Priority.DEFAULT
    ) -> _KVCache:
        return _KVCache(self, lora_task_id, input_tokens,
                        custom_priority_callback)

    # If best_efforts is True, we will try to resize the quota to the largest possible value that is still <= quota, and returns False only when we cannot resize the quota at all.
    # If best_efforts is False, we will resize the quota to the exact value of quota, and give up if not possible.
    def resize(self,
               cache_level: CacheLevel,
               quota: int,
               best_efforts: bool = False) -> bool:
        raise NotImplementedError("Not implemented")

    def get_quota(self, cache_level: CacheLevel) -> int:
        raise NotImplementedError("Not implemented")

    # sorted by CacheLevel from warm to cold
    @property
    def cache_tier_list(self) -> HomoTuple[CacheTier]:
        return self._storage.cache_tiers

    @property
    def tokens_per_block(self) -> int:
        return self._radix_tree.tokens_per_block

    @property
    def allow_seq_rebasing(self) -> bool:
        'If True, when we commit a full block, we will try to find a existing reusable block with the same tokens and reuse that block instead to save some memory. Intra-batch reuse will be enabled if this is True.'
        return True

    @property
    def enable_partial_match(self) -> bool:
        return True
